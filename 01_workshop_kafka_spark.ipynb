{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj8aXelUgZdQ"
      },
      "source": [
        "# Let's have some fun with code "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEVAFlz-QlwE"
      },
      "source": [
        "In previous introduction notebook we showed you, how simple is it to use Kafka by yourself.\n",
        "\n",
        "In order to stream PID data, we used something very similar. Downloading a data, parse them and send to broker server, which is being hosted on AWS. So basicaly we producing data and now you have to consume them. \n",
        "\n",
        "We will use pyspark and [spark structured streaming API](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html).\n",
        "\n",
        "It is not the only one option, how to consume kafka topics. Kafka itself has APIs to manage it fully, see more [here](https://kafka.apache.org/documentation/#api)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3mDGB-lgdDO"
      },
      "source": [
        "First, we have to connect to Kafka's broker. The broker is hosted as[ MSK Kafka AWS Service](https://aws.amazon.com/msk/).\n",
        "\n",
        "\n",
        "\n",
        "1.   *broker1 b-1-public.bdffelkafka.3jtrac.c19.kafka.us-east-1.amazonaws.com:9196*\n",
        "2.   *broker2  b-2-public.bdffelkafka.3jtrac.c19.kafka.us-east-1.amazonaws.com:9196*\n",
        "\n",
        "You will need user name and password, provided on previous lectures. \n",
        "\n",
        "We have 5 topics from which we can read: trams, buses, regbuses, trains and boats, we will start with trams for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUhSQTsZhSw5"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# connect to broker\n",
        "JAAS = 'org.apache.kafka.common.security.scram.ScramLoginModule required username=\"usr\" password=\"pwd\";'\n",
        "tram_stream_topic = spark.readStream \\\n",
        "  .format(\"kafka\")\\\n",
        "  .option(\"kafka.bootstrap.servers\", \"b-2-public.bdffelkafka.3jtrac.c19.kafka.us-east-1.amazonaws.com:9196, b-1-public.bdffelkafka.3jtrac.c19.kafka.us-east-1.amazonaws.com:9196\") \\\n",
        "  .option(\"kafka.sasl.mechanism\", \"SCRAM-SHA-512\")\\\n",
        "  .option(\"kafka.security.protocol\", \"SASL_SSL\") \\\n",
        "  .option(\"kafka.sasl.jaas.config\", JAAS) \\\n",
        "  .option(\"subscribe\", \"trams\") \\\n",
        "  .load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1RhAZt8l99n"
      },
      "source": [
        "It is possible to cast json messages directly,  using schema, or you can cast it just to string, but it might complicate your work later. \n",
        "\n",
        "Use schema below. It is possible to save it to other python or dbx notebook and call it externally, see example below for dbx notebook.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "%run \"./pid_schema\" # the notebook's name with function in it\n",
        "schema_pid = get_pid_schema() # use it for casting later\n",
        "```\n",
        "\n",
        "To call the schema, it must be wrapped into function with return."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I11FWfaHdPL-"
      },
      "source": [
        "The schema is uploaded on github, copy paste it to separate notebook, or right away in the same notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3IQ48TRlKrT"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql.functions import from_json, col\n",
        "base_trams = tram_stream_topic.select(from_json(col(\"value\").cast(\"string\"), schema_pid).alias(\"data\")).select(\"data.*\") \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds_1fRUYn8Za"
      },
      "source": [
        "Let's start the actual spark stream. There are several ways, how to store data, which output mode - what type of sink we will use, documentation is [here](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks).\n",
        "\n",
        "Now we use format `memory` - data will be stored in memory and we will append, ie. we do not wait for complete data (in specified batch or so).\n",
        "\n",
        "Memory sink is nice for debugging, but you have to be really sure, that data are not big too much. Data are being saved in memory table on Spark's driver.\n",
        "\n",
        "Supported mode is `Append` or `Complete`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvMXS96QoMAS"
      },
      "outputs": [],
      "source": [
        "# \n",
        "tram_stream_mem_append = base_trams.writeStream \\\n",
        "        .format(\"memory\")\\\n",
        "        .queryName(\"mem_trams\")\\\n",
        "        .outputMode(\"append\")\\\n",
        "        .start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeX5mQJepOAE"
      },
      "source": [
        "Using files is useful, when you'd like to append data and save bigger amount and process it later. \n",
        "\n",
        "You can pull hourly data from stream to your file and analyze it after few days. This can be done by running trigger option and scheduling notebook to run every hour in Workflows menu.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHg9TTSwpQg0"
      },
      "outputs": [],
      "source": [
        "tram_stream_file = base_trams.writeStream \\\n",
        "        .format(\"parquet\")\\\n",
        "        .option(\"path\", \"path/to/destination/dir\")\n",
        "        .start()\n",
        "#.trigger(once=True).format(\"delta\").queryName(query_name).outputMode(\"append\").option(\"checkpointLocation\", \"/Filestore/whatever/\").toTable(\"nameoftable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOV9kYq8iMDu"
      },
      "source": [
        "\n",
        "We can try now some basic SQL operations on the mem_trams table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoZa7gbcwR-P"
      },
      "outputs": [],
      "source": [
        "%sql\n",
        "select * from mem_trams;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMvMhF-OwcsP"
      },
      "source": [
        "We can check how many entries do we have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoQBSORuwc6e"
      },
      "outputs": [],
      "source": [
        "%sql\n",
        "select count(*) from mem_trams;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0Kegls9xKsy"
      },
      "source": [
        "Just remember, if you want to make some transformations on the data, you should not do it directly on stream, since you could corrupt the data in it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB7dasrhxhVK"
      },
      "outputs": [],
      "source": [
        "%sql\n",
        "create table data_trams select * from mem_trams;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPE3Bkw0xKhZ"
      },
      "source": [
        "Now we can do some transformation on the data_trams. For example we can print coordinations of the first 10 trams.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqqjS1fLyUuN"
      },
      "outputs": [],
      "source": [
        "%sql\n",
        "select geometry.coordinates from data_trams limit 10;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrxMTQBXkl_p"
      },
      "source": [
        "Ok, let's divide an array with coords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1mlkyA2t6Ba"
      },
      "outputs": [],
      "source": [
        "%sql\n",
        "SELECT \n",
        "  cast(data_trams.geometry.coordinates[0] as double) AS x,\n",
        "  cast(data_trams.geometry.coordinates[1] as double) AS y\n",
        "FROM data_trams;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NC81lYNwZLz"
      },
      "source": [
        "To visualize geopoints, you can use geopy, matplotlib, openstreetmaps and osmnx libs... we will be using some of them. First we need to install libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7UCqw8wFzoJ"
      },
      "outputs": [],
      "source": [
        "%pip install osmnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWTVtH2-F0l4"
      },
      "outputs": [],
      "source": [
        "%pip install numpy==1.23.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi0xCG9kF8EI"
      },
      "source": [
        "Import osmnx and create custom filter (more links you have, longer you will wait).\n",
        "You can create graph by center, by polygon etc. See [OSMNX docs](https://osmnx.readthedocs.io/en/stable/osmnx.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LoOyjCgF4Ja"
      },
      "outputs": [],
      "source": [
        "import osmnx as ox\n",
        "\n",
        "custom_filter='[\"highway\"~\"motorway|motorway_link|trunk|trunk_link|primary|primary_link|secondary|secondary_link|road|road_link\"]'\n",
        "\n",
        "G = ox.graph_from_place(\"Praha, Czechia\", custom_filter=custom_filter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNVN0XL_GNRL"
      },
      "source": [
        "Once you graph is loaded, you can plot it out together with your x and y coordintaes (obtainted via select Spark above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TXzDd2oGaBz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# this makes your plot wait and not closing\n",
        "fig, ax = ox.plot_graph(G, show=False, close=False)\n",
        "df_geo_p = df_geo.toPandas()\n",
        "# you can plot all, or some subsection for quicker result\n",
        "x = df_geo_p.loc[1:300,'x']\n",
        "y = df_geo_p.loc[1:300,'y']\n",
        "\n",
        "ax.scatter(x, y, c='red')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtnFA_QqGuVf"
      },
      "source": [
        "The results is the map below.\n",
        "\n",
        "<img src=\"https://i.imgur.com/TMeOP0M.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlAt6h6fHWAK"
      },
      "source": [
        "Have fun with geo spatial data :) \n",
        "\n",
        "<img src=\"https://i.imgur.com/1ZNplF4.png\" width=\"20%\">"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
